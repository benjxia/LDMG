{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8300c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio_augmentations as ta\n",
    "\n",
    "import lightning as L\n",
    "from lightning import LightningModule\n",
    "\n",
    "import auraloss\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import os\n",
    "\n",
    "import utility as U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2facf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_INPUT_SR = 16000\n",
    "DEFAULT_LATENT_SR = 125 # Chosen because 16000 / 2^7 = 125, and we have an even number of 0.5x downsamples\n",
    "DEFAULT_LATENT_CHANNELS = 16 # Seems to be a pretty standard value for this\n",
    "\n",
    "DEFAULT_1D_KERNEL_SIZE = 7 # This seems to be standard practice for waveforms\n",
    "DEFAULT_1D_PADDING = 3 # Padding necessary for kernel size 7 for exact halving of dimensions\n",
    "\n",
    "DEFAULT_MAX_CHANNELS = 256\n",
    "\n",
    "DEFAULT_AUDIO_DUR = 10 # In seconds\n",
    "MAX_SEQ_LEN = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c67ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELBO_Loss(nn.Module):\n",
    "    def __init__(self, KL_weight=1e-3):\n",
    "        super(ELBO_Loss, self).__init__()\n",
    "        self.stft = auraloss.freq.MultiResolutionSTFTLoss()\n",
    "        self.KL = KL_weight\n",
    "\n",
    "    def forward(self, recon_x: torch.Tensor, x: torch.Tensor, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        STFT = self.stft(recon_x, x)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return STFT + self.KL * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ede1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 channels: int,\n",
    "                 n_heads: int):\n",
    "        \"\"\"\n",
    "        Multiheaded self-attention (with residual connections)\n",
    "\n",
    "        Args:\n",
    "            channels (int): Channels for input sequence\n",
    "            n_heads (int): Number of attention heads\n",
    "        \"\"\"\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.dim = channels\n",
    "        self.attn = nn.MultiheadAttention(channels, n_heads, batch_first=True)\n",
    "\n",
    "\n",
    "    def _posn_encoding(self, seq_len: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Positional encoding\n",
    "        Args:\n",
    "            seq_len (int): Sequence length\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Positional encoding of shape [seq_len, dim]\n",
    "        \"\"\"\n",
    "        position = torch.arange(0, seq_len, 1).unsqueeze(0).unsqueeze(-1)\n",
    "        denom = torch.pow(10000, -2 * torch.arange(0, self.dim, 1) / self.dim).unsqueeze(0).unsqueeze(0)\n",
    "        pe = torch.zeros((1, seq_len, self.dim))\n",
    "        pe[:, :, 0::2] = torch.sin(position * denom[:, :, 0::2])\n",
    "        pe[:, :, 1::2] = torch.cos(position * denom[:, :, 1::2])\n",
    "        self.register_buffer('pe', pe)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for self-attention (with residual connections)\n",
    "\n",
    "        B = batch\n",
    "        C = channels\n",
    "        T = time\n",
    "        Args:\n",
    "            x (torch.Tensor): Sequence tensor of shape [B, C, T]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of shape [B, C, T]\n",
    "        \"\"\"\n",
    "        x = x.permute(0, 2, 1)  # [B, T, C]\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        embeddings = self._posn_encoding(T).to(x.device)  # [B, T, C]\n",
    "        attn_in = x + embeddings\n",
    "\n",
    "        attn_out, _ = self.attn(attn_in, attn_in, attn_in, need_weights=False)\n",
    "        out = x + attn_out  # Residual connection\n",
    "        return out.permute(0, 2, 1)  # Back to [B, C, T]\n",
    "\n",
    "class DownsampleLayer(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, activation: str='gelu'):\n",
    "        super(DownsampleLayer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, DEFAULT_1D_KERNEL_SIZE, stride=2, padding=DEFAULT_1D_PADDING)\n",
    "        self.norm = nn.GroupNorm(out_channels // 4, out_channels)\n",
    "        self.activation = U.get_activation(activation)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_channels: int,\n",
    "                 latent_channels: int=DEFAULT_LATENT_CHANNELS,\n",
    "                 input_sr: int=DEFAULT_INPUT_SR,\n",
    "                 latent_sr: int=DEFAULT_LATENT_SR):\n",
    "        \"\"\"\n",
    "        Conditional Variational Autoencoder Encoder\n",
    "        Args:\n",
    "            input_channels (int): Number of channels for input audio waveforms (ex. stereo vs. mono)\n",
    "            latent_channels (int): Number of channels for latent audio waveforms\n",
    "            input_sr (int): Input audio waveform sample rate (16000Hz default)\n",
    "            latent_sr (int): Target Latent audio sample rate (125Hz default) - No guarantees it'll actually reach this\n",
    "        \"\"\"\n",
    "        super(VAE_Encoder, self).__init__()\n",
    "\n",
    "        self.input_channels = input_channels,\n",
    "        self.latent_channels = latent_channels\n",
    "        self.input_sr = input_sr\n",
    "        self.latent_sr = latent_sr\n",
    "        self.activation = 'gelu'\n",
    "\n",
    "        # Input dimension must be some power of 2 multiple of latent dim\n",
    "        self.n_downsamples = np.ceil(np.log2(self.input_sr / self.latent_sr)).astype(np.int32)\n",
    "        assert (2 ** self.n_downsamples) * latent_sr == self.input_sr\n",
    "\n",
    "        starter_channels = 16\n",
    "        layers = [\n",
    "            nn.Conv1d(input_channels, starter_channels, DEFAULT_1D_KERNEL_SIZE, stride=1, padding=DEFAULT_1D_PADDING),\n",
    "            U.get_activation_module('gelu'),\n",
    "        ]\n",
    "\n",
    "        # Channels go from 16 -> 32 -> 64 -> DEFAULT_MAX_CHANNELS ... n_downsamples layers\n",
    "        in_ch = starter_channels\n",
    "        for i in range(self.n_downsamples):\n",
    "            out_ch = min(in_ch * 2, DEFAULT_MAX_CHANNELS)\n",
    "            layers.append(DownsampleLayer(in_ch, out_ch))  # Downsample by factor of 2\n",
    "            in_ch = out_ch\n",
    "\n",
    "        layers.append(SelfAttention(in_ch, 4))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        self.mu_proj = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, in_ch, kernel_size=DEFAULT_1D_KERNEL_SIZE, padding=DEFAULT_1D_PADDING),\n",
    "            U.get_activation_module('gelu'),\n",
    "            nn.Conv1d(in_ch, latent_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.logvar_proj = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, in_ch, kernel_size=DEFAULT_1D_KERNEL_SIZE, padding=DEFAULT_1D_PADDING),\n",
    "            U.get_activation_module('gelu'),\n",
    "            nn.Conv1d(in_ch, latent_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        VAE encoder forward pass, input waveforms are expected to be of the correct sample rate (from VAE encoder constructor)\n",
    "\n",
    "        B = batch size\n",
    "        C = channels\n",
    "        T = timesteps\n",
    "\n",
    "        L = latent space channels\n",
    "        T' = T * latent_sr / input_sr\n",
    "        Args:\n",
    "            x (torch.Tensor): Batch of waveforms of shape [B, C, T]\n",
    "        Returns:\n",
    "            Parameters to a diagonal Gaussian in latent space\n",
    "            torch.Tensor: Latent space tensor of shape [B, L, T'] mean\n",
    "            torch.Tensor: Latent space tensor of shape [B, L, T'] log variances\n",
    "        \"\"\"\n",
    "        x = self.layers(x)\n",
    "        return self.mu_proj(x), self.logvar_proj(x)\n",
    "\n",
    "class UpsampleLayer(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, activation: str='gelu'):\n",
    "        super(UpsampleLayer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.upsample = nn.ConvTranspose1d(in_channels, out_channels, DEFAULT_1D_KERNEL_SIZE, stride=2, padding=DEFAULT_1D_PADDING, output_padding=1)\n",
    "        self.conv = nn.Conv1d(out_channels, out_channels, DEFAULT_1D_KERNEL_SIZE, stride=1, padding=DEFAULT_1D_PADDING)\n",
    "        self.norm = nn.GroupNorm(out_channels // 4, out_channels)\n",
    "        self.activation = U.get_activation(activation)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.upsample(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv(x) + x\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_channels: int,\n",
    "                 latent_channels: int=DEFAULT_LATENT_CHANNELS,\n",
    "                 input_sr: int=DEFAULT_INPUT_SR,\n",
    "                 latent_sr: int=DEFAULT_LATENT_SR):\n",
    "        \"\"\"\n",
    "        Conditional Variational Autoencoder Decoder\n",
    "        Args:\n",
    "            input_channels (int): Number of channels for input audio waveforms (ex. stereo vs. mono)\n",
    "            latent_channels (int): Number of channels for latent audio waveforms\n",
    "            input_sr (int): Input audio waveform sample rate (16000Hz default)\n",
    "            latent_sr (int): Target Latent audio sample rate (125Hz default) - No guarantees it'll actually reach this\n",
    "        \"\"\"\n",
    "        super(VAE_Decoder, self).__init__()\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.latent_channels = latent_channels\n",
    "        self.input_sr = input_sr\n",
    "        self.latent_sr = latent_sr\n",
    "        self.activation = 'gelu'\n",
    "\n",
    "        # Input dimensions must be some power of 2 multiple of latent dim\n",
    "        self.n_upsamples = np.ceil(np.log2(self.input_sr / self.latent_sr)).astype(np.int32)\n",
    "        assert (2 ** self.n_upsamples) * latent_sr == self.input_sr\n",
    "\n",
    "        channels = DEFAULT_MAX_CHANNELS\n",
    "        layers = [\n",
    "            nn.Conv1d(latent_channels, channels, DEFAULT_1D_KERNEL_SIZE, stride=1, padding=DEFAULT_1D_PADDING),\n",
    "            U.get_activation_module('gelu'),\n",
    "        ]\n",
    "\n",
    "        layers.append(SelfAttention(channels, 4))\n",
    "\n",
    "        for i in range(self.n_upsamples):\n",
    "            layers.append(UpsampleLayer(channels, channels))\n",
    "\n",
    "        layers.append(nn.Conv1d(channels, input_channels, kernel_size=1))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        VAE decoder forward pass, input latent space waveforms and outputs waveforms in original input space\n",
    "\n",
    "        B = batch size\n",
    "        Z = latent channels\n",
    "        T = timesteps\n",
    "        Args:\n",
    "            x (torch.Tensor): Batch of latent waveforms of shape [B, Z, T']\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Reconstruction of waveforms in input space of shape [B, C, T]\n",
    "        \"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, audio_channels: int):\n",
    "        super(VAE, self).__init__()\n",
    "        self.channels = audio_channels\n",
    "        self.encoder = VAE_Encoder(audio_channels)\n",
    "        self.decoder = VAE_Decoder(audio_channels)\n",
    "        self.latent_dim = self.decoder.latent_channels\n",
    "        self.latent_sr = self.decoder.latent_sr\n",
    "\n",
    "    def _sample(self, mu: torch.Tensor, log_var: torch.Tensor):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def generate(self, n_samples: int=1) -> torch.Tensor:\n",
    "        z = torch.randn([n_samples, self.latent_dim, self.latent_sr * DEFAULT_AUDIO_DUR])\n",
    "        audio = self.decoder(z)\n",
    "        return audio\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Full VAE encoder + decoder forward pass\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Batch of waveforms of shape [B, C, T]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Reconstruction of waveforms in input space of shape [B, C, T]\n",
    "            torch.Tensor: Mean of Gaussian distribution over latent space\n",
    "            torch.Tensor: Log variance of Gaussian distribution over latent space\n",
    "\n",
    "        \"\"\"\n",
    "        mu, log_var = self.encoder(input)\n",
    "        sample = self._sample(mu, log_var)\n",
    "        reconstruction = self.decoder(sample)\n",
    "        return reconstruction, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f89e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 32, 15, stride=4, padding=7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(32, 64, 15, stride=4, padding=7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(64, 128, 15, stride=4, padding=7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv1d(128, 1, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).mean(dim=-1)  # shape [B, 1]\n",
    "\n",
    "class AudioVAEGAN(LightningModule):\n",
    "    def __init__(self, channels: int, kl_weight: float = 1e-3, adv_weight: float = 1.0, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.vae = VAE(channels)\n",
    "        self.discriminator = Discriminator(channels)\n",
    "\n",
    "        self.recon_loss = ELBO_Loss(kl_weight)\n",
    "        self.adv_weight = adv_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vae(x)\n",
    "\n",
    "    def adversarial_loss(self, pred, target_is_real=True):\n",
    "        target = torch.ones_like(pred) if target_is_real else torch.zeros_like(pred)\n",
    "        return F.binary_cross_entropy_with_logits(pred, target)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        real = batch\n",
    "\n",
    "        opt_vae, opt_disc = self.optimizers()\n",
    "\n",
    "        ### === Train Generator (VAE Decoder) === ###\n",
    "        self.toggle_optimizer(opt_vae)\n",
    "        recon, mu, logvar = self.vae(real)\n",
    "\n",
    "        elbo = self.recon_loss(recon, real, mu, logvar)\n",
    "        d_fake = self.discriminator(recon)\n",
    "        adv_loss = self.adversarial_loss(d_fake, target_is_real=True)\n",
    "        total_gen_loss = elbo + self.adv_weight * adv_loss\n",
    "\n",
    "        self.manual_backward(total_gen_loss)\n",
    "        opt_vae.step()\n",
    "        opt_vae.zero_grad()\n",
    "        self.untoggle_optimizer(opt_vae)\n",
    "\n",
    "        ### === Train Discriminator === ###\n",
    "        self.toggle_optimizer(opt_disc)\n",
    "        with torch.no_grad():\n",
    "            recon_detached, _, _ = self.vae(real)\n",
    "\n",
    "        d_real = self.discriminator(real)\n",
    "        d_fake = self.discriminator(recon_detached)\n",
    "\n",
    "        real_loss = self.adversarial_loss(d_real, target_is_real=True)\n",
    "        fake_loss = self.adversarial_loss(d_fake, target_is_real=False)\n",
    "        d_loss = 0.5 * (real_loss + fake_loss)\n",
    "\n",
    "        self.manual_backward(d_loss)\n",
    "        opt_disc.step()\n",
    "        opt_disc.zero_grad()\n",
    "        self.untoggle_optimizer(opt_disc)\n",
    "\n",
    "        ### === Logging === ###\n",
    "        self.log_dict({\n",
    "            \"gen_elbo\": elbo,\n",
    "            \"gen_adv\": adv_loss,\n",
    "            \"gen_total\": total_gen_loss,\n",
    "            \"disc_loss\": d_loss\n",
    "        }, prog_bar=True, on_step=True, on_epoch=True)\n",
    "\n",
    "    def generate(self, n_samples: int):\n",
    "        return self.vae.generate(n_samples)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_g = torch.optim.Adam(self.vae.parameters(), lr=self.hparams.lr)\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=self.hparams.lr * 2)\n",
    "        return [opt_g, opt_d]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44dd42c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioVAE(LightningModule):\n",
    "    def __init__(self, channels: int, kl_weight: float = 1e-3, lr=1e-4):\n",
    "        super(AudioVAE, self).__init__()\n",
    "        self.vae = VAE(channels)\n",
    "        self.loss = ELBO_Loss(kl_weight)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.vae(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx=None, dataloader_idx=None) -> torch.Tensor:\n",
    "        reconstruction, mu, log_var = self.vae(batch)\n",
    "        loss = self.loss(reconstruction, batch, mu, log_var)\n",
    "        self.log('training_elbo_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def generate(self, n_samples: int):\n",
    "        return self.vae.generate(n_samples)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.vae.parameters(), self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a6cec",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTZANAudioDataset(Dataset):\n",
    "    def __init__(self, path: str, sample_rate=16000, duration=10):\n",
    "        \"\"\"\n",
    "        Expects a path to .../Data - i.e. the path should end with \"Data\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        if not os.path.exists(path):\n",
    "            import requests\n",
    "            import zipfile\n",
    "            # Stream the download to avoid loading the whole file into memory\n",
    "            with requests.get('https://www.kaggle.com/api/v1/datasets/download/andradaolteanu/gtzan-dataset-music-genre-classification', stream=True) as r:\n",
    "                r.raise_for_status()  # Raise an error on bad status\n",
    "                with open('gtzan.zip', 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "\n",
    "            # Ensure the extract_to directory exists\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "\n",
    "            # Extract the ZIP file\n",
    "            with zipfile.ZipFile('gtzan.zip', 'r') as zip_ref:\n",
    "                path = os.path.pardir(path)\n",
    "                zip_ref.extractall(path)\n",
    "\n",
    "        self.df = pd.read_csv(os.path.join(path, 'features_30_sec.csv'))\n",
    "        self.min_audio_len = self.df['length'].min()\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "        # Filter malformed shit\n",
    "        self.df = self.df[self.df['filename'] != 'jazz.00054.wav']\n",
    "        self.resampler = torchaudio.transforms.Resample(22050, sample_rate)\n",
    "\n",
    "        self.sr = sample_rate\n",
    "        self.target_frames = self.sr * duration\n",
    "\n",
    "        self.crop = ta.RandomResizedCrop(self.target_frames)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        genre = self.df.iloc[index]['label']\n",
    "        filename = self.df.iloc[index]['filename']\n",
    "        path = os.path.join(self.path, 'genres_original', genre, filename)\n",
    "        waveform, sr = torchaudio.load(path, normalize=True)\n",
    "\n",
    "        if sr != self.sr:\n",
    "            waveform = self.resampler(waveform)\n",
    "\n",
    "        if waveform.size(1) < self.target_frames:\n",
    "            pad_len = self.target_frames - waveform.size(1)\n",
    "            waveform = F.pad(waveform, (0, pad_len))\n",
    "        else:\n",
    "            waveform = self.crop(waveform)\n",
    "\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52f687f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = \"./\",\n",
    "        batch_size: int = 1,\n",
    "        num_workers: int = 1,\n",
    "        target_sr: int = 16000,\n",
    "        clip_duration: float = 10.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.target_sr = target_sr\n",
    "        self.clip_duration = clip_duration\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        self.train_dataset = GTZANAudioDataset(\n",
    "            self.data_dir,\n",
    "            16000,\n",
    "            10\n",
    "        )\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        clips = []\n",
    "        for clip in batch:\n",
    "            clips.append(clip)\n",
    "        return torch.stack(clips)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, collate_fn=self._collate_fn\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c43018c",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72953536",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = MusicDataModule(\"/home/benjx/cs_wsl/school/y4/cse253/LDMG/Data\", batch_size=1, num_workers=1)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29f482d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/benjx/anaconda3/envs/ldmg/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\n",
      "  | Name | Type      | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | vae  | VAE       | 10.1 M | train\n",
      "1 | loss | ELBO_Loss | 0      | train\n",
      "-------------------------------------------\n",
      "10.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.1 M    Total params\n",
      "40.490    Total estimated model params size (MB)\n",
      "80        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/benjx/anaconda3/envs/ldmg/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/999 [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vae = AudioVAE(1)\n",
    "\n",
    "trainer = L.Trainer()\n",
    "trainer.fit(vae, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "def show_wav(waveform, sample_rate):\n",
    "    display(Audio(waveform, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a44811",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = dm.train_dataset[0].unsqueeze(0).to('cuda:0', dype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ad4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = vae.to(device='cuda:0')\n",
    "with torch.no_grad():\n",
    "    out, _, _ = vae(tmp)\n",
    "out = out[0][0].detach().cpu().numpy()\n",
    "print(np.max(out))\n",
    "\n",
    "# Example waveform and sample rate\n",
    "sample_rate = 16000  # in Hz\n",
    "waveform = out\n",
    "\n",
    "show_wav(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073798e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tmp[0][0].detach().cpu().numpy()\n",
    "print(np.max(out))\n",
    "\n",
    "# Example waveform and sample rate\n",
    "sample_rate = 16000  # in Hz\n",
    "waveform = out\n",
    "\n",
    "show_wav(waveform, sample_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldmg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
